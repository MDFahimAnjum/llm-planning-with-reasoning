{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Intrinsic Evaluation\n",
    "\n",
    "We measure the discrimination abilities of LLMs with four intrinsic metrics:\n",
    "\n",
    "1. **Discrimination Accuracy (Acc)**: Given a pair of correct and wrong programs, we calculate the percentage where the correct program obtains a higher discrimination score than the wrong one.\n",
    "\n",
    "2. **Classification Macro F1 (F1)**: We treat \"correct\" and \"wrong\" as two classes and compute the macro average of F1 scores on these two labels.\n",
    "\n",
    "3. **Hit@1 (H@1)**: Given a batch of candidate programs, we calculate the percentage where the highest-scoring candidate is correct.\n",
    "\n",
    "4. **Mean Reciprocal Rank (MRR)**: We compute the standard MRR score by the highest-ranking correct program in the batches.\n",
    "\n",
    "\n",
    "Legacy script: `scripts\\intrin_eval\\intrin_eval_text2sql_ft.sh`\n",
    "\n",
    "### Datasets:\n",
    "1. [Spider](https://yale-lily.github.io/spider)\n",
    "\n",
    "#### Spider Dataset Keys Explanation\n",
    "\n",
    "The Spider dataset contains various keys that help in evaluating text-to-SQL models. Below is an explanation of each key:\n",
    "\n",
    "- **`db_id`**: The unique identifier of the database for the given query. This indicates which database schema the question belongs to.\n",
    "\n",
    "- **`schema`**: The schema of the database, which includes information about tables, columns, and their relationships. This helps models understand the database structure.\n",
    "\n",
    "- **`question`**: The natural language question asked by the user.  \n",
    "  *Example:*  \n",
    "  *\"What is the name of the youngest employee?\"*\n",
    "\n",
    "- **`sql`**: The ground truth SQL query corresponding to the question.  \n",
    "  *Example:*  \n",
    "  ```sql\n",
    "  SELECT name FROM employees ORDER BY age ASC LIMIT 1;\n",
    "  ```\n",
    "\n",
    "- **`exec_res`**: The execution result of the ground truth SQL query. This contains the actual output of running the query on the database.\n",
    "\n",
    "- **`top_n`**: A list of the **top-N SQL completions** (candidate queries) generated by the model. These are ranked based on the model’s confidence scores.\n",
    "\n",
    "- **`top_n_exec_res`**: The execution results of the **top-N SQL completions**. These contain the actual database outputs of the model’s predicted queries.\n",
    "\n",
    "- **`top_n_label`**: A list of binary labels (`0` or `1`) for each SQL candidate in `top_n`.  \n",
    "  - `1` → The query is **correct** (produces the expected output).  \n",
    "  - `0` → The query is **incorrect** (does not produce the expected output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from modules.llm_evaluator import LLMEvaluator, LLMLoraEvaluator, LLMReasoningEvaluator\n",
    "from utils.functions import set_seed_all\n",
    "from utils.functions import eval_intrinsic\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_names =[\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # generation\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"stabilityai/stable-code-3b\", # generation\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-instruct\", # generation\n",
    "    \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "]\n",
    "\n",
    "# generate lora model names\n",
    "lora_model_names = []\n",
    "for m in evaluator_names:\n",
    "   lora_model_names.append( m.split(\"/\")[1]+\"_spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator_name: codellama/CodeLlama-13b-Instruct-hf\n",
      "evaluator_lora: CodeLlama-13b-Instruct-hf_spider_NoExecResult\n",
      "evaluator_type: base\n",
      "evaluator setup: check\n",
      "FT training type: NoExecResult\n",
      "Use Schema: False\n",
      "evaluator_max_new_tokens: 512\n",
      "evaluation_config = {'check_exec': True, 'use_exec_res': False}\n",
      "{'evaluator_name': 'codellama/CodeLlama-13b-Instruct-hf', 'evaluator_lora': 'CodeLlama-13b-Instruct-hf_spider_NoExecResult', 'evaluator_type': 'base', 'evaluator_setting': 'check', 'evaluation_config': {'check_exec': True, 'use_exec_res': False}, 'FT training type:': 'NoExecResult', 'Use Schema in prompt': False, 'seed': 42, 'test_fname': 'data/spider_intrin_eval.json', 'evaluator_faliValue': -0.5, 'evaluator_max_new_tokens': 512}\n"
     ]
    }
   ],
   "source": [
    "model_indx = 6 # choose the model to evaluate\n",
    "evaluator_type = 'base' # 'base' / 'FT' / 'reason'\n",
    "evaluator_setting = 'check' # 'base', 'check', 'exec', 'pro'\n",
    "ft_type = 'NoExecResult' # 'NoExecResult' / 'withExecResult'\n",
    "useSchema = False # Use schema as context for evaluator\n",
    "evaluator_faliValue = -0.5\n",
    "evaluator_max_new_tokens = 512\n",
    "\n",
    "evaluator_name = evaluator_names[model_indx]\n",
    "model_savename = lora_model_names[model_indx] + \"_\" + ft_type # + '_b256_e1'\n",
    "\n",
    "print(f\"evaluator_name: {evaluator_name}\")\n",
    "print(f\"evaluator_lora: {model_savename}\")\n",
    "print(f\"evaluator_type: {evaluator_type}\")\n",
    "print(f\"evaluator setup: {evaluator_setting}\")\n",
    "print(f\"FT training type: {ft_type}\")\n",
    "print(f\"Use Schema: {useSchema}\")\n",
    "print(f\"evaluator_max_new_tokens: {evaluator_max_new_tokens}\")\n",
    "\n",
    "current_directory = os.getcwd() #parameters\n",
    "model_savedatapath = os.path.join(current_directory,f\"checkpts/{model_savename}/model\")\n",
    "evaluator_peft_dir = model_savedatapath\n",
    "\n",
    "seed = 42\n",
    "curr_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#test_fname = \"data/spider_intrin_eval.json\"\n",
    "test_fname = \"data/bird_intrin_eval.json\"\n",
    "# test_fname = \"data/spider_evaluator_train.json\"\n",
    "log_name = f\"{model_savename}_{curr_timestamp}.json\"\n",
    "dataset_name = \"spider\"\n",
    "db_path =\"data/spider/database\"\n",
    "evaluation_config = f\"configs/{evaluator_setting}.json\"\n",
    "print(f\"evaluation_config = {json.load(open(evaluation_config))}\")\n",
    "\"\"\"\n",
    "yes_token_indx: \n",
    "    the index of the token in the vocabulary that corresponds to the \"Yes\" text.\n",
    "    CodeLlama-Instruct: \"No\" 1939 \"Yes\" 3869\n",
    "    TinyLlama: \"Yes\" 3869\n",
    "\"\"\"\n",
    "yes_token_indx=None#3869\n",
    "\n",
    "params_log={\n",
    "   \"evaluator_name\": evaluator_name,\n",
    "   \"evaluator_lora\": model_savename,\n",
    "   \"evaluator_type\": evaluator_type,\n",
    "   \"evaluator_setting\": evaluator_setting,\n",
    "   \"evaluation_config\": json.load(open(evaluation_config)),\n",
    "   \"FT training type:\": ft_type,\n",
    "   \"Use Schema in prompt\": useSchema,\n",
    "   \"seed\": seed,\n",
    "   \"test_fname\": test_fname,\n",
    "   \"evaluator_faliValue\": evaluator_faliValue,\n",
    "   \"evaluator_max_new_tokens\": evaluator_max_new_tokens\n",
    "}\n",
    "\n",
    "print(params_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "set_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfd5bdc86d24849b2fdc55562cc3170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded base model\n"
     ]
    }
   ],
   "source": [
    "if evaluator_type == 'base':\n",
    "    evaluator = LLMEvaluator(evaluator_name, db_path, device=\"cuda\",yes_token_indx=yes_token_indx)\n",
    "    print(f\"Loaded base model\")\n",
    "elif evaluator_type == 'FT':\n",
    "    evaluator = LLMLoraEvaluator(evaluator_name, evaluator_peft_dir, db_path, device=\"cuda\",yes_token_indx=yes_token_indx)\n",
    "    print(f\"Loaded LoRA FT model\")\n",
    "else:\n",
    "    evaluator = LLMReasoningEvaluator(base_model_name=evaluator_name, db_path=db_path, device=\"cuda\",\\\n",
    "                                       failvalue=evaluator_faliValue, max_new_tokens=evaluator_max_new_tokens)\n",
    "    print(f\"Loaded for reasoning\")\n",
    "# yindx=evaluator.get_yes_token()\n",
    "# print(f\"Yes token index: {yindx}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/78746073/how-to-solve-torch-was-not-compiled-with-flash-attention-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/400 [00:00<?, ?it/s]c:\\Users\\fahim\\anaconda3\\envs\\r1\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:53: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 400/400 [25:39<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair Count: 409\n",
      "PWS Acc: 0.7995              \n",
      "SQL Count: 1221\n",
      "Pos F1: 0.0000              \n",
      "Neg F1: 0.7419              \n",
      "Macro F1: 0.3709              \n",
      "Hit @ 1: 0.6475              \n",
      "MRR: 0.6777              \n",
      "\n",
      "PWS Acc\tPos F1\tNeg F1\tMacro F1\tHit@1\tMRR\n",
      "\n",
      "0.7995              ,\t0.0000              ,\t0.7419              ,\t0.3709              ,\t0.6475              ,\t0.6777              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_intrinsic(evaluator, test_fname, evaluation_config, log_fname=log_name, useSchema=useSchema,params_log=params_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear all variables\n",
    "%reset -f\n",
    "\n",
    "# Clear memory for PyTorch (if using GPU)\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Garbage collection to free up memory\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
